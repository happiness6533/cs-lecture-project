## 컴퓨터 네트워크
##############################################################
nat 뭔지 찾을 것
http

v 1.0
단기 커넥션. 각각의 HTTP 요청은 각각의 커넥션 상에서 실행된다. 이는 TCP 핸드 셰이크는 각 HTTP 요청 전에 발생하고, 이들이 직렬화됨을 의미한다. TCP 핸드셰이크는 그 자체로 시간을 소모하기는 하지만 TCP 커넥션은 지속적으로 연결되었을 때 부하에 맞춰 더욱 예열되어 더욱 효율적으로 작동한다. 단기 커넥션들은 TCP의 이러한 효율적인 특성을 사용하지 않게 하며 예열되지 않은 새로운 연결을 통해 지속적으로 전송함으로써 성능이 최적 상태보다 저하된다. 1) 즉, 새로운 연결을 맺는데 드는 시간이 상당하다
또한 TCP 기반 커넥션의 성능은 오직 커넥션이 예열된 상태일 때만 나아지는데 이를 활용하지 못했다.

이런 문제를 완화시키기 위해 영속적인 커넥션의 컨셉이 만들어졌다. 이는 keep-alive 커넥션이라고 불리기도 한다. 연결을 열어놓고 여러 요청에 재사용함으로써, 새로운 TCP 핸드셰이크를 하는 비용을 아끼고 TCP의 성능 향상 기능을 활용할 수 있다. 커넥션은 영원히 열려있지는 않으며 유휴 커넥션들은 얼마 후에 닫힌다.(서버는 Keep-Alive 헤더를 사용해서 연결이 최소한 얼마나 열려있어야 할지를 설정할 수 있다.)

영속적인 커넥션도 단점을 가지고 있다. 유휴 상태일때에도 서버 리소스를 소비하며 과부하 상태에서는 DoS attacks을 당할 수 있다. 이런 경우에는 커넥션이 유휴 상태가 되자마자 닫히는 비영속적 커넥션(non-persistent connections)을 사용하는 것이 더 나은 성능을 보일 수 있다.
HTTP/1.0 커넥션은 기본적으로 영속적이지 않다. 

v 1.1
Persistent Connection이라는 개념을 도입하게 된다. 이는 지정한 timeout 동안 커넥션을 닫지 않는 방식이다. 즉, 특정 시간동안 여러 요청이 한 커넥션을 사용할 수 있는 것이다. HTTP/1.1에서는 기본적으로 영속적이며 이를 위한 헤더도 필요하지 않다. HTTP/1.1에서는 Connection 헤더가 close 값으로 설정되어 전송된 경우에만 단기 커넥션으로 사용된다.

클라이언트의 요청이 올때만 서버가 응답을 하는 구조로 매 요청마다 connection을 생성해야만 합니다. cookie 등 많은 메타 정보들을 저장하는 무거운 header가 요청마다 중복 전달되어 비효율적이고 느린 속도를 보여주었습니다.

Pipeline이라는 기법이 도입이 된다. 추가적으로 도입된 Pipeline 같은 경우는 다음과 같은 문제를 해결하기 위함이다. HTTP 요청들 같은 경우엔 순차적으로 응답을 받아야되는데 첫 번째로 들어온 요청을 응답해줘야 다음으로 들어오는 두 번째 요청을 받을 수 있고 두 번째 요청을 응답해줘야 세 번째 요청을 받을 수 있다는 것이다. 이는 대기시간이 길어진다. 파이프라인은 에서 응답을 기다리지 않고 순차적인 여러 요청을 연속적으로 보내 그 순서에 맞춰 응답을 받는 방식으로 지연 시간을 줄인다. 파이프라인은 에서 응답을 기다리지 않고 순차적인 여러 요청을 연속적으로 보내 그 순서에 맞춰 응답을 받는 방식으로 지연 시간을 줄인다.
그런데 이 파이프라인에는 치명적인 문제가 있었다. 그것은 바로 Head Of Line Blocking이라는 것이다. 이는 첫 번째 요청이 왔는데 서버에서 처리하는 시간이 너무 오래 걸려서 두 번째 요청, 세 번째 요청이 기다려야 하는 것이다. 만약 두 번째 요청이 0.1초만에 끝나는 요청이여도 첫 번째 요청이 끝나지 않았다면 처리될 수 없는 것이다.
또다른 문제는 Header 구조의 중복이다. 요청이 연속적으로 이루어질 때 헤더의 값이 중복되는게 있지만 그 데이터를 중복적으로 보내게 된다는 비효율적인 측면이 있다. 그래서 주고받는 데이터가 쓸데없이 커지게 되는 것이다. http 1.1의 헤더에는 많은 메타정보들이 저장된다. 매 요청 시 마다 중복된 Header 값을 전송하게 되며 또한, 해당 domain에 설정된 cookie 정보도 매 요청 시 마다 헤더에 포함되어 전송된다.(헤더 중복). HTTP는 클라이언트가 여러 개의 커넥션을 맺음으로써 여러 개의 HTTP 트랜잭션을 병렬로 처리할 수 있게 한다.
병렬 커넥션은 페이지를 더 빠르게 내려받는다. 하나의 커넥션으로 객체들을 로드할 때의 대역폭 제한과 대기시간을 줄일 수 있다면 더 빠르게 로드할 수 있다.
하지만, 병렬 커넥션이 항상 더 빠르지는 않다. 왜냐하면 클라이언트의 네트워크 대역폭이 좁을 때는 대부분 시간을 데이터 전송하는 데만 쓸 것이다. 그리고 다수의 커넥션은 메모리를 많이 소모하고 자체적인 성능 문제를 발생시킨다. 브라우저는 실제로 병렬 커넥션을 사용하긴 하지만 적은 수(대부분 4개, 최신 브라우저는 6~8개)의 병렬 커넥션만을 허용한다. 서버는 특정 클라이언트로부터 과도한 수의 커넥션이 맺어졌을 경우 그것을 임의로 끊어버릴 수 있다.
병렬 커넥션이 실제로 페이지를 더 빠르게 내려받는 것은 아니지만 화면에 여러 개의 객체가 동시에 보이면서 내려받고 있는 상황을 볼 수 있기 때문에 사용자는 더 빠르게 내려받고 있는 것 처럼 느낄 수 있다.

그리고 HTTP 1.1에서는 RTT(Round Trip Time)의 증가라는 단점이 있다. 일반적으로 Connection 하나에 요청 한 개를 처리하는데 이렇다보니 매번 요청 별로 Connection을 만들게 되고 TCP상에서 동작하는 HTTP의 특성상 3-way Handshake가 반복적으로 일어나며, 불필요한 RTT증가와 네트워크 지연을 초래하여 성능일 지연시킨다.

기타
Image Spriting: 웹 페이지를 구성하는 다양한 아이콘 이미지 파일의 요청 횟수를 줄이기 위해 아이콘을 하나의 큰 이미지로 만든 다음 CSS에서 해당 이미지의 자표 값을 지정하여 표시하는 것
Minified CSS/Javascript: HTTP를 통해 전송되는 데이터의 용량을 줄이기 위해서 CSS. Javascript를 축소하여 적용한다. name.min.js, name.min.css 등이 그 예이다.

HTTP 파이프라이닝
기본적으로 HTTP 요청은 순차적이다. 현재의 요청에 대한 응답을 받고 나서야 다음 요청을 실시한다. 네트워크 지연과 대역폭 제한에 걸려 다음 요청을 보내는 데까지 상당한 딜레이가 발생할 수 있다.
파이프라이닝이란 같은 영속적인 커넥션을 통해서 응답을 기다리지 않고 요청을 연속적으로 보내는 기능이다. 이것은 커넥션의 지연을 회피하고자 하는 방법이다. 이론적으로는 두 개의 HTTP 요청을 하나의 TCP 메시지 안에 채워서 성능을 더 향상시킬 수 있다. HTTP 요청의 사이즈는 지속적으로 커져왔지만 일반적인 MSS(최대 세그먼트 크기)는 몇 개의 간단한 요청을 포함하기에는 충분히 여유 있다.
모든 종류의 HTTP의 요청이 파이프라인으로 처리될 수 있는 것은 아니다. GET, HEAD, PUT 그리고 DELETE 메서드 같은 idempotent 메서드만 가능하다. 실패가 발생한 경우에는 단순히 파이프라인 컨텐츠를 다시 반복하면 된다.
오늘 날, 모든 HTTP/1.1 호환 프록시와 서버들은 파이프라닝을 지원해야 하지만 실제로는 많은 프록시 서버들은 제한을 가지고 있다. 모던 브라우저가 이 기능을 기본적으로 활성화하지 않은 이유다.

v2
한 connection으로 동시에 여러 개 메시지를 주고 받으며, header를 압축하여 중복 제거 후 전달하기에 version1에 비해 훨씬 효율적입니다. 또한, 필요 시 클라이언트 요청 없이도 서버가 리소스를 전달할 수도 있기 때문에 클라이언트 요청을 최소화 할 수 있습니다.

HTTP 2.0의 특징을 보자. 그 중에 가장 중요한 것은 아래와 같다.
HTTP 메시지 전송 방식의 변화
바이너리 프레이밍 계층 사용 - 파싱, 전송 속도 증가, 오류 발생 가능성 감소
바이너리 프레이밍이 Application 계층 안에 추가된 것을 주의깊게 보자. 
HTTP 1.1에서는 텍스트 형식의 메시지를 보냈다면 HTTP 2.0에서는 Frame 단위로 데이터를 분할한다.(HEADERS frame, DATA frame)
심지어 분할 후 이를 바이너리로 인코딩한다. 그렇게 해서 데이터를 보내게 된다. 바이너리로 인코딩을 하게 됨으로 파싱이나 전송 속도가 증가하고 오류가 발생할 가능성이 감소하는 것이다. 새 바이너리 프레이밍 매커니즘이 도입됨에 따라 클라이언트와 서버 간의 데이터 교환 방식이 바뀌었다. 이 과정을 설명하기 위해 HTTP 2.0 용어를 살펴보자.

스트림: 일반적으로 데이터,패킷,비트 등의 일련의 연속성을 갖는 흐름을 의미를 가진다. 음성, 영상, 데이터 등의 작은 조각들이 하나의 줄기를 이루며 전송되는 데이터 열을 의미한다. 호스트 상호 간 또는 동일 호스트 내 프로세스 상호간 통신에서 큐에 의한 메세지 전달방식을 이용한 가상 연결 통로를 의미하기도 한다.
메시지: 논리적 요청 또는 응답 메시지에 매핑되는 프레임의 전체 시퀀스이다.
프레임: HTTP 2.0 에서 통신의 최소 단위이며 각 최소 단위에는 하나의 프레임 헤더가 포함된다. 이 프레임 헤더는 최소한으로 프레임이 속하는 스트림을 식별한다.

이러한 용어의 관계는 다음과 같이 요약된다.
모든 통신은 단일 TCP 연결을 통해 수행되며 전달될 수 있는 양방향 스트림의 수는 제한이 없다.
각 스트림에는 양방향 메시지 전달에 사용되는 고유 식별자와 우선순위 정보(선택 사항)가 있다.
각 메시지는 하나의 논리적 HTTP 메시지(예: 요청 또는 응답)이며 하나 이상의 프레임으로 구성된다.
프레임은 통신의 최소 단위이며 특정 유형의 데이터(예: HTTP 헤더, 메시지 페이로드 등)를 전달합니다. 다른 스트림들의 프레임을 인터리빙한 다음, 각 프레임의 헤더에 삽입된 스트림 식별자를 통해 이 프레임을 다시 조립할 수 있습니다.
위 사진에서 보는 것과 스트림 안에서 전송되는 각각의 프레임들은 합쳐져 하나의 응답이나 요청 이 되는 것이다. HTTP/2는 HTTP 프로토콜 통신을 바이너리 인코딩된 프레임의 교환으로 세분화한다. 그런 다음 이 프레임은 특정 스트림에 속하는 메시지에 매핑되며, 모든 프레임은 단일 TCP 연결 내에서 다중화된다. HTTP/2 프로토콜이 제공하는 다른 모든 기능과 성능 최적화는 이러한 기반을 통해 지원된다.

HTTP/2의 새 바이너리 프레이밍 계층은 전체 요청 및 응답 다중화를 지원한다. 이를 위해 클라이언트와 서버가 HTTP 메시지를 독립된 프레임으로 세분화하고, 이 프레임을 인터리빙한 다음, 다른 쪽에서 다시 조립하도록 허용한다. 위 사진은 동일한 연결 내의 여러 스트림을 캡처한 것이다. 클라이언트는 DATA 프레임(스트림 5)을 서버로 전송 중인 반면, 서버는 스트림 1과 스트림 3의 인터리빙된 프레임 시퀀스를 클라이언트로 전송 중이다. 따라서 3개의 병렬 스트림이 존재한다.

HTTP 2.0의 또다른 특징으로는 Stream 우선순위가 있다.
HTTP 메시지가 많은 개별 프레임으로 분할될 수 있고 여러 스트림의 프레임을 다중화(Multiplexing)할 수 있게 되면서 스트림들의 우선순위를 지정할 필요가 생겼다. 클라이언트는 우선순위 지정을 위해 ‘우선순위 지정 트리’를 사용하여 서버의 스트림처리 우선순위를 지정할 수 있다. 서버는 우선순위가 높은 응답이 클라이언트에 우선적으로 전달될 수 있도록 대역폭을 설정한다.
같은 레벨에서는 가중치 비율에 따라 가중치가 매겨 지며, 부모는 자식들에 대비하여, 자식들의 가중치 비율을 합한 만큼의 가중치 비율을 가진다.

HTTP 2.0 특징 정리
Multiplexed Streams: Connection 한 개로 동시에 여러 개의 메시지를 주고 받을 수 있으며 응답은 순서에 상관없이 Stream으로 주고 받음
Stream Priortization: 문서 내에 CSS 파일 1개와 이미지 파일 2개가 존재하고 이를 클라이언트가 요청하는 상황에서 이미지 파일보다 CSS 파일의 수신이 늦어진다면 브라우저 렌더링에 문제가 생긴다. HTTP/2에서는 이러한 상황을 고려하여 리소스 간의 의존관계에 따른 우선순위를 설정하여 리소스 로드 문제를 해결
Server Push: 서버는 클라이언트가 요청하지 않은 리소스를 사전에 푸쉬를 통해 전송할 수 있다. 이렇게 리소스 푸쉬가 가능해지면 클라이언트가 추후에 HTML 문서를 요청할 때 해당 문서 내의 리소스를 사전에 클라이언트에서 다운로드할 수 있도록 하여 클라이언트의 요청을 최소화할 수 있다.
Header Compression: HTTP/2는 헤더 정보를 압축하기 위해 Header Table과 Huffman Encoding 기법을 사용하여 처리하는데 이를 HPACK 압축방식이라 부른다
위 그림처럼 클라이언트가 요청을 두 번 보낸다고 가정할 때 HTTP/1.x의 경우 헤더 중복이 발생해도 중복 전송한다. 하지만 HTTP/2 에서는 헤더 중복이 있는 경우 Static/Dynamic Header Table 개념을 이용하여 중복을 검출해내고 해당 테이블에서의 index 값+ 중복되지 않은 Header 정보를 Huffman Encoding 방식으로 인코딩한 데이터를 전송한다.
##############################################################
##############################################################
dns
로컬 dns > 외부 dns 차례대로 조회
##############################################################
cname

ip가 유동적으로 변하는 서버의 경우 그 서버의 별명 도메인과 실제 도메인을 연결할 떄 쓴다
##############################################################
공인ip 사설 ip

사설 ip로 서버 운영하기
포트포워딩 방법 = 공인 ip에 포트를 개방해서 내부의 사설 ip에 하나씩 연결
dmz 방법 = 공유기의 모든 포트를 내부의 특정 사설ip에 다 몰아줌
##############################################################
고정ip 유동ip

고정 ip는 비싸기 때문에, 유동 ip는 네트워크 업체에서 현재 활성화중인 네트워크에만 ip를 주고 나머지는 걷어가는 방식으로, 한국은 현재 약 1억개의 ip를 가지고 이를 활용중이다.

이렇게 되면 사설ip로 서버를 운영하는게 불가능해 지기 때문에 다른 방법을 사용해야 한다. 그게 바로 ddns. 즉, ip가 유동적으로 바뀌어도 동일한 도메인으로 연결해 준다. 공유기 페이지에서 설정 가능!
##############################################################
TCP와 UDP의 차이
연결 지향 방식이다.
3-way handshaking과정을 통해 연결을 설정하고 4-way handshaking을 통해 해제한다.
흐름 제어 및 혼잡 제어.
높은 신뢰성을 보장한다.
UDP보다 속도가 느리다.
전이중(Full-Duplex), 점대점(Point to Point) 방식.
TCP가 연결 지향 방식이라는 것은 패킷을 전송하기 위한 논리적 경로를 배정한다는 말입니다. 그리고 3-way handshaking과정은 목적지와 수신지를 확실히 하여 정확한 전송을 보장하기 위해서 세션을 수립하는 과정을 의미합니다. TCP가 이러한 특징을 지니는 이유는 간단명료합니다. 바로 TCP는 연결형 서비스로 신뢰성을 보장하기 때문입니다. 그래서 3-way handshaking의 과정도 사용하는 것이고, 데이터의 흐름제어나 혼잡 제어와 같은 기능도 합니다. 하지만 이러한 기능때문에 UDP보다 속도가 느리게 됩니다.(이러한 기능은 CPU를 사용하기 때문에 속도에 영향을 주는 것입니다.) 그렇기에 TCP는 연속성보다 신뢰성있는 전송이 중요할 때에 사용하는 프로토콜로 예를 들면 파일 전송과 같은 경우에 사용됩니다!
서버소켓은 연결만을 담당한다.
연결과정에서 반환된 클라이언트 소켓은 데이터의 송수신에 사용된다
서버와 클라이언트는 1대1로 연결된다.
스트림 전송으로 전송 데이터의 크기가 무제한이다.
패킷에 대한 응답을 해야하기 때문에(시간 지연, CPU 소모) 성능이 낮다.
Streaming 서비스에 불리하다.(손실된 경우 재전송 요청을 하므로)
Q) 패킷(Packet)이란?
 
인터넷 내에서 데이터를 보내기 위한 경로배정(라우팅)을 효율적으로 하기 위해서 데이터를 여러 개의 조각들로 나누어 전송을 하는데 이때, 이 조각을 패킷이라고 합니다.
 
Q) TCP는 패킷을 어떻게 추적 및 관리하나요?
 위에서 데이터는 패킷단위로 나누어 같은 목적지(IP계층)으로 전송된다고 설명하였습니다. 예를 들어 한줄로 서야하는 A,B,C라는 사람(패킷)들이 서울(발신지)에서 출발하여 부산(수신지)으로 간다고 합시다. 그런데 A,B,C가 순차적으로 가는 상황에서 B가 길을 잘못 들어서 분실되었다고 합시다. 하지만 목적지에서는 A,B,C가 모두 필요한지 모르고 A,C만 보고 다 왔다고 착각할 수 있습니다. 그렇기 때문에 A,,B,C라는 패킷에 1,2,3이라는 번호를 부여하여 패킷의 분실 확인과 같은 처리를 하여 목적지에서 재조립을 합니다. 이런 방식으로 TCP는 패킷을 추적하며, 나누어 보내진 데이터를 받고 조립을 할 수 있습니다.

2. UDP 
       데이터를 데이터그램 단위로 처리하는 프로토콜 
여기서 데이터그램이란 독립적인 관계를 지니는 패킷이라는 뜻으로, UDP의 동작방식을 설명하자면 다음과 같습니다. 위에서 대충 눈치채셨듯이 TCP와 달리 UDP는 비연결형 프로토콜입니다. 즉, 연결을 위해 할당되는 논리적인 경로가 없는데, 그렇기 때문에 각각의 패킷은 다른 경로로 전송되고, 각각의 패킷은 독립적인 관계를 지니게 되는데 이렇게 데이터를 서로 다른 경로로 독립적으로 처리하게 되고, 이러한 프로토콜을 UDP라고 합니다. 
비연결형 서비스로 데이터그램 방식을 제공한다
정보를 주고 받을 때 정보를 보내거나 받는다는 신호절차를 거치지 않는다.
UDP헤더의 CheckSum 필드를 통해 최소한의 오류만 검출한다.
신뢰성이 낮다
TCP보다 속도가 빠르다
 
UDP는 비연결형 서비스이기 때문에, 연결을 설정하고 해제하는 과정이 존재하지 않습니다. 서로 다른 경로로 독립적으로 처리함에도 패킷에 순서를 부여하여 재조립을 하거나 흐름 제어 또는 혼잡 제어와 같은 기능도 처리하지 않기에 TCP보다 속도가 빠르며 네트워크 부하가 적다는 장점이 있지만 신뢰성있는 데이터의 전송을 보장하지는 못합니다. 그렇기 때문에
신뢰성보다는 연속성이 중요한 서비스 예를 들면 실시간 서비스(streaming)에 자주 사용됩니다.
UDP에는 연결 자체가 없어서(connect 함수 불필요) 서버 소켓과 클라이언트 소켓의 구분이 없다.
소켓 대신 IP를 기반으로 데이터를 전송한다.
서버와 클라이언트는 1대1, 1대N, N대M 등으로 연결될 수 있다.
데이터그램(메세지) 단위로 전송되며 그 크기는 65535바이트로, 크기가 초과하면 잘라서 보낸다.
흐름제어(flow control)가 없어서 패킷이 제대로 전송되었는지, 오류가 없는지 확인할 수 없다.
파일 전송과 같은 신뢰성이 필요한 서비스보다 성능이 중요시 되는 경우에 사용된다.
 
Q) 흐름제어(Flow Control)와 혼잡제어(Congestion Control)이란?
흐름제어는 데이터를 송신하는 곳과 수신하는 곳의 데이터 처리 속도를 조절하여 수신자의 버퍼 오버플로우를 방지하는 것입니다. 예를 들어 송신하는 곳에서 감당이 안되게 데이터를 빠르게 많이 보내면 수신자에서 문제가 발생하기 때문입니다.
혼잡제어는 네트워크 내의 패킷 수가 넘치게 증가하지 않도록 방지하는 것입니다. 만약 정보의 소통량이 과다하면
패킷을 조금만 전송하여 혼잡 붕괴 현상이 일어나는 것을 막습니다. 
  
 
 
[ TCP Flow ]
 

 
 
[ UDP Flow ]

##############################################################
node가 tcp 데이터를 완전체로 받아서 작동하는 원리
##############################################################
Serialization

파일에 텍스트를 기록하고, 이진 데이터를 기록하는 방법은 많이들 알고 있다.
그런데 만약 이런 종류의 데이터들이 아니라 객체를 파일로 저장하거나 읽어올 수 있을까? 있다!!! 직렬화가 그것을 가능하게 해준다.
객체화하여 파일이나 네트워크로 write할 때는 직렬화를 거쳐서 전달된다.
반대로 읽어올때는 역직렬화(Deserialization)를 거쳐서 가져오게 된다.
구글 프로토버프랑 연결해서 이를 설명해보자
##############################################################
Web Server

WAS가 해야 할 일의 부담을 줄이기 위해서
WAS 앞에 웹 서버를 둬서 웹 서버에서는 정적인 문서만 처리하도록 하고, WAS는 애플리케이션의 로직만 수행하도록 기능을 분배하여 서버의 부담을 줄이기 위한 것이다.
WAS의 환경설정 파일을 외부에 노출시키지 않도록 하기 위해서
클라이언트와 연결하는 포트가 직접 WAS에 연결이 되어 있다면 중요한 설정 파일들이 노출될 수 있기 때문에 WAS 설정 파일을 외부에 노출시키지 않도록 하기 위해서 웹 서버를 앞단에 배치시킨다. 웹 서버와 WAS에 접근하는 포트가 다르기 때문에, WAS에 들어오는 포트에는 방화벽을 쳐서 보안을 강화할 수도 있다.

자바 웹 애플리케이션을 개발할 때 주로 사용하는 조합이 아파치와 톰캣일 것이다. 그러면 다른 언어들은 톰캣 같은 WAS가 없을까?
예를 들어 아파치에 PHP 모듈을 설치했을 경우, 요청이 왔을 때 아파치는 HTTP 헤더를 분석하고 파싱하여 PHP로 파라미터를 넘겨준다. 그러면 PHP에서는 파라미터를 받아 응답 할 HTML 문서를 만들어서 아파치에 전달한다. HTML 문서를 전달 받은 아파치는 CSS, JS, img 등 정적인 자원들과 함께 브라우저로 반환한다.
그런데 자바는 CGI로 구현되어 있지 않다. 자바 자체가 무겁고, Common 라이브러리와 JEE라는 플랫폼이 존재하기 때문에 아파치에서 굳이 CGI를 제공하지 않는 것 같다. 그렇기 때문에 톰캣은 Default Servlet을 통해 정적인 파일을 제공해주기 때문에 웹 서버의 역할을 할 수 있는 것이다.
##############################################################
CGI

CGI는 사용자가 요청한 정보가 정적인 HTML 파일이 아니라 PHP, Python에서 요청이 오게 되면 웹서버는 자신이 처리할 수 없다는 사실을 알고 PHP 인터프리터에게 의뢰를 해서 개발자가 작성한 PHP 스크립터를 읽고 처리하여 그 결과를 웹서버에게 돌려주게 되고 웹서버는 그것을 다시 브라우저에게 돌려주게 된다. 이렇게 웹서버(Nginx, Apache)와 PHP, Python 사이에 존재하여 규격화된 약속으로 서로 데이터를 전송하여 처리하는 것이 CGI이다. CGI는 요청할 때마다 프로세스를 생성하고 프로세스가 가동하면서 시스템 자원을 소비하게 된다. 또한, 동시에 많은 요청이 발생하면 프로세스가 생성되면서 서버에 부하가 발생하게 된다. FastCGI는 기존 CGI처럼 요청마다 프로세스를 생성하는 것이 아닌 1개의 큰 프로세스에 생성해서 여러 요청을 처리하게 된다. 1개의 프로세스만으로 처리하여 여러 프로세스를 생성해서 실행하는 부하를 해결하게 된다.
##############################################################
WAS

DB 조회나 다양한 로직 처리를 요구하는 동적인 컨텐츠를 제공하기 위해 만들어진 Application Server
HTTP를 통해 컴퓨터나 장치에 애플리케이션을 수행해주는 미들웨어(소프트웨어 엔진)이다.
분산 트랜잭션, 보안, 메시징, 쓰레드 처리 등의 기능을 처리하는 분산 환경에서 사용된다.
주로 DB 서버와 같이 수행된다.
현재는 WAS가 가지고 있는 Web Server도 정적인 컨텐츠를 처리하는 데 있어서 성능상 큰 차이가 없다
Load Balancing을 위해서 Web Server를 사용할 수 있다.
fail over(장애 극복), fail back처리에 유리하다. 특히 대용량 웹 애플리케이션의 경우(여러 개의 서버 사용) Web Server와 WAS를 분리하여 무중단 운영을 위한 장애 극복에 쉽게 대응할 수 있다. 예를 들어, 앞 단의 Web Server에서 오류가 발생한 WAS를 이용하지 못하도록 한 후 WAS를 재시작함으로써 사용자는 오류를 느끼지 못하고 이용할 수 있다.
하나의 서버에서 PHP Application과 Java Application을 함께 사용하는 경우가 있다.

접근 허용 IP관리 및 2대 이상의 서버에서의 세션 관리 등도 Web Server에서 처리하면 효율적이다.
Web Server를 WAS 앞에 두고 필요한 WAS들을 Web Server에 플러그인 형태로 설정하면 더욱 효율적인 분산 처리가 가능하다.
##############################################################
서브넷 마스크

서브넷 마스크의 개념에 앞서 서브네팅(Sub-netting)의 개념에 대해 먼저 알아야 한다. 서브넷팅은 하나의 주 네트워크(Major Network)를 필요한 만큼 분할하여 상호 연결 망을 구축할 수 있게 해주는 것이다. IP 주소의 고갈 문제를 해결하기 위해 설계된 개념이고 반대로 축약하는 개념은 수퍼넷팅(Super-netting)이라고 한다.

예를 들어, 회사에 영업부(50명), 회계부(50명), 관리부(50명) 등이 존재하는데 이를 하나의 네트워크로 묶을 수도 있지만 이를 분할해서 네트워크를 구분 지어준다. 이 구분된 네트워크를 브로드캐스트 도메인(Broadcast Domain)이라고 하고 너무 큰 브로드캐스트 도메인은 자원을 효율적으로 분배하기 어려워서 속도가 느려진다.
따라서, 서브넷팅으로 네트워크를 분할하게 되면 필요한 네트워크 주소만 호스트 IP로 할당해서 쓸모없이 버려지는 잉여 IP를 최소화할 수 있으며 Broadcast Domain Size를 줄여주므로 LAN의 Traffic을 줄일 수 있다. 또한 여러 네트워크로 분할되므로 분할된 각 network(서로 다른 부서) 업무 간에 보안을 유지할 수 있다.
##############################################################
서브넷 마스크란 IP Address에서 첫비트부터 어디까지가 네트워크 부분인가 알려주는 역할이다. 쉽게 생각하면 IP 주소에 마스크를 씌워서 어디까지가 네트워크 부분인가를 표시하는 것이다. IP Address처럼 32비트로 구성되며, 네트워크 부분을 표시하는 비트는 1, 호스트 부분은 </span>을 표시하는 비트는 0이다. 또한,연속성이 존재해서 네트워크 부분 중간에 0이 들어갈 수 없다.
예를 들어, IP 주소 192.168.1.1에 서브넷 마스크가 255.255.255.0라면 255로 표시된 부분인 192.168.1.까지는 네트워크 부분이고 0으로 표시된 부분인 .1은 호스트 부분이다. 다시 말해, 255는 이진법으로 표시하면 1111111이기 때문에 네트워크 부분인 것이다.

 네트워크 부분과 호스트 부분을 구하는 TIP!
서브네팅한 네트워크 부분을 확인할 때는 IP주소와 Subnet mask를 AND 연산해서 구한다. AND 연산은 각 비트를 비교해서 모두 1인 경우에만 1 값을 반환한다.



255.255.255.0 서브넷 마스크의 경우 24 bit가 네트워크 부분이며, prefix 표기법으로 /24이다.
AND 연산을 통해서 네트워크 부분을 구하게 될 경우 Network Address를 구할 수 있으며, 이때 호스트 부분을 모두 1로 바꿀 경우 Broadcast Address를 구할 수 있다. 이 두가지 주소의 경우 호스트에 할당할 수 없으며 이를 제외한 나머지 IP주소가 호스트에 할당 가능한 주소(Assigned Address)이다.
따라서 위의 네트워크를 보면, 192.168.1.1/24는 Network Address가 192.168.1.0이고 Broadcast Address가 192.168.1.255이며 할당 가능한 호스트는 192.168.1.1~192.168.1.254까지이다. 호스트 수는 254개 이며 이를 구하는 공식은 2의 호스트 비트수 제곱에서 2를 뺀 값이다.

 문제를 통한 이해

 IP: 10.0.24.100 Subnet Mask: 255.0.0.0 (= 10.0.25.100/8)
Network Address: 10.0.0.0
Broadcast Address: 10.255.255.255
할당 가능한 호스트 주소: 10.0.0.1 ~ 10.255.255.254
호스트 개수: 2^24-2 = 16777216-2 = 16777214

 IP: 192.100.2.31/16
Network Address: 192.100.0.0
Broadcast Address: 192.100.255.255
할당 가능한 호스트 주소: 192.100.0.1 ~ 192.100.255.254
호스트 개수: 2^16-2 = 65536-2 = 65534

 IP: 151.3.192.17 Subnet Mask: 255.255.240.0 (= 151.3.192.17/20)
Network Address: 151.3.192.0
Broadcast Address: 151.3.207.255
할당 가능한 호스트 주소: 151.3.192.1 ~ 151.3.207.254
호스트 개수: 2^12-2=4096-2 = 4094

 IP: 192.168.4.100/26
Network Address: 192.168.4.64
Broadcast Address: 192.168.4.127
할당 가능한 호스트 주소: 192.168.4.65 ~ 192.168.4.126
호스트 개수: 2^6-2 = 64-2 = 66

 마지막 문제

192.168.4.100/26를 비트화 시켜보면

11000000 10101000 00000100 01100100
11111111 11111111 11111111 1100000
11000000 10101000 00000100 01 까지 네트워크 부분이고 나머지 6비트는 호스트이다.

그래서 호스트 비트를 0으로 채운 192.168.4.64가 네트워크 주소(Network Address)이고 1로 채운
11000000 10101000 00000100 01111111 (192.168.4.127)는 브로드캐스트 주소(Broadcast Address)이다.
따라서 네트워크 주소와 브로드캐스트 주소 사이에 있는 IP 주소들이 호스트에게 할당가능한 주소(192.168.4.65 ~ 192.168.4.126)이다. 또한, 호스트 개수는 호스트 비트가 6bit 이기 때문에 2의 6제곱 빼기 2 = 64 - 2 = 62개이다.

마지막으로 한가지만 더 짚어보자
192.168.4.100은 192로 시작하기 때문에 C 클래스이다. 그러면 네트워크 부분이 192.168.4.0/24 이렇게 된다. 그러면 해당 네트워크에는 호스트 수가 .4.1부터 .4.254까지 254개가 된다.
그.런.데 위의 문제는 네트워크 부분이 2비트가 늘어났다. 자 이렇게 서브네팅을 하게되면
192.168.4.0 ~ 192.168.4.255
이렇게 하나였던 네트워크를
192.168.4.0 ~ 192.168.4.63
192.168.4.64 ~ 192.168.4.127
192.168.4.128 ~ 192.168.4.191
192.168.4.192 ~ 192.168.4.254

이렇게 4개 네트워크로 분할하게 된다.
위 문제에 있던 .100 이라는 호스트는 두 번째 네트워크에 해당된다.
하나의 네트워크 일때는 호스트가 254개인데 4개의 네트워크로 분할하면 각 네트워크당 62개씩으로 할당된다. 더 효율적으로 활용이 가능하게 된 것이다. 이처럼 서브넷팅은 분할에 사용하며 반대로 네트워크를 합칠 때는 축약이라고 하며 수퍼넷팅이라고 한다.




0.0.0.0의 의미

책으로 웹 개발을 하다보면 아주 가끔 0.0.0.0이라는 IP를 사용할 때가 있다.
해당 IP를 그저 모든 IP에 접근하기 위해 사용한다 정도로만 알고 있었다. 하지만 왜 서버 측에서 0.0.0.0으로 접근하는데 local의 웹 서버로 접근이 되는지 이해가 되지 않았다.
이에 대한 좋은 영문 글이 있다 What is the Difference Between 127.0.0.1 and 0.0.0.0?

이 문구를 보면, 0.0.0.0은 local machine의 모든 IPv4 address를 의미하기 때문에 0.0.0.0로 접근하면 로컬 호스트의 모든 IPv4로 되어있는 호스트에 접근이 가능하다는 의미이다. 조금 더 설명을 붙이자면, 정확한 address가 할당되어 있지 않다면, 각각의 host는 그 address를 자신이라고 주장하게 되고 이에 따라 웹 서비스에서 0.0.0.0을 지정하면 자신의 IP를 그 address로 지정하게 되어 local로 접근이 되는 것이다.
##############################################################
Web Server의 기능
HTTP 프로토콜을 기반으로 하여 클라이언트(웹 브라우저 또는 웹 크롤러)의 요청을 서비스 하는 기능을 담당
웹 서버의 가장 중요한 기능은 클라이언트가 요청하는 HTML 문서나 각종 리소스를 전달하는 것
정적인 컨텐츠 제공
WAS를 거치지 않고 바로 자원을 제공함
동적인 컨텐츠 제공을 위한 요청 전달
클라이언트의 요청(Request)을 WAS에 보내고, WAS가 처리한 결과를 클라이언트에게 전달(응답, Response)한다.
클라이언트는 일반적으로 웹 브라우저를 의미한다.

ex: Apache, Nginx, Microsoft, Google 웹서버이다.


 웹 애플리케이션 서버(WAS)

브라우저 DBMS(데이터 베이스 관리 시스템) 사이에서 동작하는 미들웨어이다.
DB 조회나 다양한 로직 처리를 요구하는 동적인 컨텐츠를 제공하기 위해 만들어진 Application Server
“웹 컨테이너(Web Container) ” 혹은 “서블릿 컨테이너(Servlet Container)”라고도 불린다

여기서 미들웨어란, 클라이언트와 DBMS 사이에서 중개 역할을 하는 미들웨어이다.

클라이언트는 단순히 미들웨어에게 요청을 보내고, 미들웨어에서는 대부분의 로직을 수행한다.

데이터를 조작할 일이 있으면 미들웨어가 DBMS에 접속하기도 한다.
WAS의 기능
프로그램 실행 환경과 DB 접속 기능 제공
여러 개의 트랜젝션(논리적인 작업 단위) 관리 기능
업무를 처리하는 비즈니스 로직 수행

 Static Pages와 Dynamic Pages



Static Pages

Web Server는 파일 경로 이름을 받아 경로와 일치하는 file contents를 반환한다.
항상 동일한 페이지를 반환한다.
ex) image, html, css, javascript 파일과 같이 컴퓨터에 저장되어 있는 파일들

Dynamic Pages

인자의 내용에 맞게 동적인 contents를 반환한다.
즉, 웹 서버에 의해서 실행되는 프로그램을 통해서 만들어진 결과물 -> Servlet: WAS 위에서 돌아가는 Java Program
개발자는 Servlet에 doGet()을 구현한다.



 Web Server와 WAS를 구분하는 이유



Web Server가 필요한 이유
클라이언트(웹 브라우저)에 이미지 파일(정적 컨텐츠)을 보내는 과정을 생각해보자
이미지 파일과 같은 정적인 파일들은 웹 문서(HTML 문서)가 클라이언트로 보내질 때 함께 가는 것이 아니다.
클라이언트는 HTML 문서를 받고 그에 맞게 필요한 이미지 파일들을 다시 서버로 요청하면 그때서야 이미지 파일을 받아온다.
Web Server를 통해 정적인 파일들을 Application Server까지 가지 않고 앞단에서 빠르게 보내줄 수 있다.
따라서 Web Server에서는 정적 컨텐츠만 처리하도록 기능을 분배하여 서버의 부담을 줄일 수 있다.

WAS가 필요한 이유
웹 페이지는 정적 컨텐츠와 동적 컨텐츠가 모두 존재한다.
사용자의 요청에 맞게 적절한 동적 컨텐츠를 만들어서 제공해야 한다.
이때, Web Server만을 이용한다면 사용자가 원하는 요청에 대한 결과값을 모두 미리 만들어 놓고 서비스를 해야 한다.
하지만 이렇게 수행하기에는 자원이 절대적으로 부족하다.
따라서 WAS를 통해 요청에 맞는 데이터를 DB에서 가져와서 비즈니스 로직에 맞게 그때그때 결과를 만들어서 제공함으로써 자원을 효율적으로 사용할 수 있다.

그렇다면 WAS가 Web Server의 기능도 모두 수행하면 되지 않을까?
기능을 분리하여 서버 부하 방지
WAS는 DB 조회나 다양한 로직을 처리하느라 바쁘기 때문에 단순한 정적 컨텐츠는 Web Server에서 빠르게 클라이언트에 제공하는 것이 좋다.
WAS는 기본적으로 동적 컨텐츠를 제공하기 위해 존재하는 서버이다.
만약 정적 컨텐츠 요청까지 WAS가 처리한다면 정적 데이터 처리로 인해 부하가 커지게 되고, 동적 컨텐츠의 처리가 지연됨에 따라 수행 속도가 느려진다.
즉, 이로 인해 페이지 노출 시간이 늘어나게 될 것이다.



물리적으로 분리하여 보안 강화
SSL에 대한 암호화 및 복호화 처리에 Web Server를 사용



여러 대의 WAS를 연결 가능
Load Balancing을 위해서 Web Server를 사용
fail over(장애 극복), fail back 처리에 유리
특히 대용량 웹 애플리케이션의 경우 (여러 개의 서버 사용) Web Server와 WAS를 분리하여 무중단 운영을 위한 장애 극복에 쉽게 대응할 수 있음
예를 들어, 앞 단의 Web Server에서 오류가 발생한 WAS를 이용하지 못하도록 한 후 WAS를 재시작함으로써 사용자는 오류를 느끼지 못하고 이용할 수 있음



여러 웹 애플리케이션 서비스 가능
예를 들어, 하나의 서버에서 PHP Application과 Java Application을 함께 사용하는 경우



기타
접근 허용 IP 관리, 2대 이상의 서버에서의 세션 관리 등도 Web Server에서 처리하면 효율적

자원 이용의 효율성 및 장애 극복, 배포 및 유지보수의 편의성을 위해 Web Server와 WAS를 분리한다.

Web Server를 WAS 앞에 두고 필요한 WAS들을 Web Server에 플러그인 형태로 설정하면 더욱 효율적인 분산 처리가 가능하다.

서버에 대해서 지식
아파치 엔진엑스 iis가 대표적인 웹서버임
서버 컴퓨터의 어떤 폴더를 외부로 개방해서 파일을 서빙해준다

정적웹
아파치 엔진엑스

동적웹은?
아파치, 엔진엑스(php 해석 모듈 세팅) php 디비를 결합해서 할 수 있다 = apm

톰캣=was=php보다 더 고도화된, 로직 처리하는 부분임
요즘은 스프링 부트에 톰캣이 내장되어 있음
자바랑 jsp로 만든 웹을 실행하면 톰캣같은 was가 실행된다
요즘은 톰캣도 정적 파일 서비스 속도가 빨라져서 별 차이 없음
그럼에도 불구하고 노드나 닷넷처럼
was가 뒷단에 있고 	웹서버가 앞단에 있어서, 아파치나 엔진엑스가 요청을 받는 이유는
아파치랑 엔진엑스는 정적 리소스 제공 말고도 다양한 기능을 제공하기 때문임
그 중 1. 리버스 프록시: 서버 내부의 구조같은걸 요청으로부터 감추기 위해서
2. 로드밸런싱: was가 여러개 돌고 있을 떄 분산해서 요청을 나눔, 또한 업데이트시 하나씩만 바꿔칠 수 있음.
3. 리버스 프록시의 캐시: 요청에 자주 등장하는 리소스를 쌓아두었다가 바로 제공
4. was가 잘 돌아가는지 헬스체크
아파치는 멀티 프로세스 모듈 방식 = 손님마다 프로세스, 손님마다 쓰레드
엔진엑스는 이벤트 방식
노드같은 경우는
어플리케이션이 was 역할까지 함

1. 현대의 컴퓨터 네트워크는 5개의 계층으로 구성된다

    - 어플리케이션 레이어: 메세지 = 헤더 + 데이터
    - 트랜스포트 레이어: 세그먼트 = 헤더 + 메세지
    - 네트워크 레이어: 패킷 = 헤더 + 세그먼트
    - 링크 레이어: 프레임 = 헤더 + 패킷
    - 피지컬 레이어: 보내고자 하는 정보를 전기 신호로 전환

2. 어플리케이션 레이어
    - 서로 다른 2개의 프로세스끼리 파이프 또는 소켓을 사용해서 데이터를 주고 받는다
    - 소켓 프로그래밍
        - os는 tcp 소켓과 udp 소켓을 지원한다
        - tcp 소켓을 사용해서 연결이 생성되는 과정
            - 서버
                - tcp 또는 udp 방식으로 소켓을 열고 포트에 바인딩한다
                - 백로그 숫자만큼 소켓을 생성하고 클라이언트의 접속을 리슨하며 무한 대기한다
            - 클라이언트
                - tcp 또는 udp 방식으로 소켓을 열고 포트에 바인딩한다
                - 서버에 커넥트한다
            - 3 way hand shaking
            - 프로토콜에 따라 데이터를 교환한다
            - 종료되면 양쪽 모두 소켓을 닫는다
        - http 프로토콜
            - tcp 프로토콜을 전제로 메세지의 헤더를 http 프로토콜에 맞게 작성하고 http 관련 데이터를 전송하는 프로토콜
            - 커넥션을 1회 사용하고 해제한다
        - socket 프로토콜
            - tcp 프로토콜을 전제로 메세지의 헤더에 socket 프로토콜에 맞게 작성하고 양방향으로 데이터를 전송하는 프로토콜
            - 한 번 맺은 커넥션을 지속적으로 사용한다

3. 트랜스포트 레이어
    - 멀티플렉싱과 디멀티플렉싱
        - 보내는 컴퓨터
        - 컴퓨터에는 많은 프로세스가 있고, 각각의 프로세스가 사용중인 소켓이 있는데
        - 다양한 소켓에서 전달해 주는 데이터를 모두 동일한 방식으로 세그먼트 > 패킷 > 데이터그램화 하는 것을 멀티플렉싱이라 한다

        - 받는 컴퓨터
        - 소켓을 통해 받은 데이터를 어플리케이션 레이어에서 정확하게 각각의 프로세스들에게 나누어 주는 것을 디멀티플렉싱이라 한다

        - 작동 원리
        - tcp의 경우
            - 네트워크 레이어에서 헤더를 검사하면 받은 패킷의 헤더에 출발지, 목적지의 아이피가 모두 적혀있다
            - 트랜스포트 레이어에서 헤더를 검사하면 받은 세그먼트의 헤더에 출발지, 목적지의 포트가 모두 적혀있다
            - 출발지와 목적지의 아이피와 포트가 모두 일치하는 소켓을 찾아서 정확한 프로세스에 데이터를 전달한다
            - 각각의 소켓을 담당하는 여러 쓰레드가 동시성으로 이러한 일들을 처리하고 있다
        - udp의 경우
            - 네트워크 레이어에서 헤더를 검사하면 받은 패킷의 헤더에 출발지, 목적지의 아이피가 모두 적혀있다
            - 트랜스포트 레이어에서 헤더를 검사하면 받은 세그먼트의 헤더에 출발지, 목적지의 포트가 모두 적혀있다
            - udp는 아이피만 같으면 데이터를 전달한다

    - 에러, 유실, 효율성, 유연성
        1. rdt 1.0: 유실
            - 에러 해결
            - 패킷을 한 번에 하나씩 전송
            - 패킷에 에러가 나는 경우를 아래의 방법으로 해결
                - 에러 체크썸
                - 피드백
                - 재전송
                - 시퀀스(0, 1) 넘버
        2. rdt 2.0: 효율성
            - 유실 해결
            - 패킷이 유실되는 경우를 대비
            - 타이머
        3. rdt 3.0 = rdt 1.0 + rdt 2.0
        4. go back n: 효율성
            - 효율 문제를 해결했다
            - 센더
                - 윈도우 사이즈 n개만큼 보냄
                - 그게 10 11 12 13이라고 치자
                - n개 전부 타이머 있음
                - n개중 하나라도 타이머 터지면 전부 재전송
                - 만약에 ack11을 받으면 윈도우를 12부터 시작하는걸로 슬라이딩함
                - 근데 ack11을 계속 보내니까
                - 12번 타이머가 결국 터짐
                - 그럼 12번부터 다시 윈도우 전부 재전송
                - 만약 정상 ack 받으면 윈도우가 바로바로 슬라이딩
            - 리시버
                - ack11 = 0부터 11까지 다 잘 받았음...12번 줘
                - 이 때 만약 13번 패킷이 먼저 도착하면?
                - 먼저 13번을 받으면 될텐데...안그럼
                - 13, 14, 15... 다 버림
                - 그리고 계속 12번 달라고 ack11을 계속 함
                - 하...
            - 센더
            - 만약 임의의 k번쨰 패킷이 유실되면
            - 거기에서 윈도우 사이즈만큼 보냇던걸
            - n만큼 뒤로 돌아와서 k번쨰부터 다시 전송
            - 문제점
                - 유실된건 12번뿐인데
                - 그거 하나떔에 n개를 다시 보내라고?
        5. selective repeat: 효율성
            - 효율 문제를 개선했다
            - 센더
                - 윈도우 사이즈 n개만큼 보냄
                - 그게 10 11 12 13이라고 치자
                - n개 전부 타이머 있음
                - 만약에 ack11을 받으면 윈도우를 12부터 시작하는걸로 슬라이딩
                -
                - 근데 ack11을 계속 보내니까
                - 12번 타이머가 결국 터짐
                - 그럼 12번만 재전송
                - 만약 정상 ack 받으면 윈도우가 바로바로 슬라이딩
            - 리시버
                - ack11 = 11번은 확실히 잘 받았음... 12번 줘
                - 이 때 만약 13번 패킷이 먼저 도착하면?
                - 이번엔 잘 받아서 버퍼에 넣어둠
                - 그리고 ack도 보내줌

            - 대체 번호를 몇 번까지 써야되냐...
            - 윈도우 사이즈만큼!!
            - 그럼 윈도우 사이즈를 4를 쓴다고 생각해보자
            - 그럼 시퀀스는 0 1 2 3이면 될텐데
            - 그럼 0 1 2 유실 유실 유실 유실 3
            - 이 문제 없는 것 처럼 느껴질 수 있다
            - 윈도우사이즈의 2배정도를 시퀀스 넘버로 쓰자!

            - 문제점
                - 모든 곳에 타이머를 달아야 됨
        6. tcp tcp는 한 쌍의 소켓끼리의 통신을 지원한다 = 포인트 투 포인트 순서대로 한다 한꺼번에 간다 full duflex 재전송할려면 센더에게도 버퍼가 있어야 한다 윈도우 사이즈 크기로
           리시브에게 버퍼가 있다 => 순서를 맞추려고 즉 각자가 2개씩 버퍼를 가짐 콘제스쳔 콘트롤 타이머 1개 쓴다 = go bacn n이랑 비슷 tcp는 타이머 터지면 그거에 헤당하는 거 하나만
           재전송한다
            1. tcp 세그먼트 구조
                - 목적지 포트번호
                - 도착지 포트번호
                - 시퀀스 번호
                    - 어플리케이션 레이어에서 내려온 데이터의 첫 번쨰 바이트 순서를 적어둠
                    - 이 번호는 보내는 자가 만들고 받는 사람은 그걸 따른다
                - 에크 넘버
                    - ack10 = 9번까지는 완벽하게 잘 받음 10번 줘
                    - 이 번호는 리시브 버퍼를 보고 요청하는 거니까 받는 사람이 만든 번호에 따라 결정됨
                    - tcp는 500ms를 기다렸다가 혹시 내가 보낼 데이터가 생기면 그 데이터 안에 이 에크를 실어서 보냄
                    - tcp 에크는 큐뮬레이티브 에크니까 500ms 기다렸다가 상황 보고 에크를 보내야 이득
                    - 받는 쪽에서 만약 ack10이 반복적으로 오면 유실 가능성이 높으므로 타이머 터지기 전에 미리 재전송
                        - 이걸 fast retransmit라고 함 = 권고사항정도임
                - 유실을 방지하기 위한 타임아웃
                    - rtt = 갔다 오는 시간 = 그떄마다 너무 다름
                    - rtt의 지수이동평균 + 마진
                - 체크썸
                - 리시브 윈도우
                - 리시브버퍼사이즈 10바이트
                    - tcp의 플로우 컨트롤
                    - 센더가 뭘 보낼 때 리시버가 받을 수 있는 만큼만 보내야 겠다
                    - 리시버가 센더에게 남은 공간이 얼마다라는걸 알려줘야댐
                    - 마안약에 리시버가 남은 공간이 0이라고 보내버리면?
                    - 센더는 흠... 하고 그떄부터 주기적으로 세그먼트에 헤더만 쓰고 데이터는 비워서 테스트로 보내봄
                    - 테스트해보니까 리시버가 드디어 준비가 됫다! 하면 이제 정상적으로 다시 보냄

            2. tcp의 3웨이 핸드 쉐이크
                1. 클라이언트가 syn segment를 보냄
                    1. 데이터는 없고 헤더에 syn이라는 1비트 필드 만들어 보냄
                    2. 나의 시퀀스 넘버가 뭐다 라고 상대에게 헤더를 통해 알려준다
                2. syn ack
                    1. syn 헤더 세팅해서 보냄
                    2. 받는 서버가 이제 자신의 시퀀스 넘버도 돌려줌
                3. ack
                    1. syn 필드는 0임
                    2. 데이터를 포함하고 있음
                        1. http라는걸 알려줄 수 있음
                    3. 왜 이걸 하냐?
                        1. 만약 2웨이라 하자 = 서버는 클라가 대체 된건지 안된건지 알 수가 없음
                4. 이게 완료 되면 이제 양쪽 다 버퍼를 만듬
                5. 커넥션을 끊을 떄도 비슷한 방식을 쓴다
                    1. fin을 보냄
                    2. 서버가 ack를 하고 자기 데이터 다 보낸 다음 역시 fin을 보냄
                    3. 클라가 확인용도로 ack 보냄 / 3초정도 쓰던 리소스와 자료구조를 유지함
                        1. 왜냐면 만약에 마지막에 보낸 ack가 유실된 경우 재전송해야되니까

            3. tcp의 콘제스쳔 컨트롤
                1. 네트워크가 나에게 알려줘(큐의 상황들을)
                    1. 이건 에바임 라우터 바빠
                2. 엔드 투 엔드 방식
                    1. 유추방식
                    2. ack의 동작 방식을 보고 유추한다
